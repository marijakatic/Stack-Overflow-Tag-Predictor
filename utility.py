# -*- coding: utf-8 -*-
"""utility.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j8FtHfBP1Q_M6YYXpzP5wvYQyuDWpo8d

# Utility

## Necessary downloads and library imports
"""

import sklearn.metrics as metrics
from keras.metrics import Accuracy

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import GridSearchCV

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense

def print_metrics(y_true, y_predicted):
    keras_acc = Accuracy()
    keras_acc.update_state(y_true, y_predicted)
    print("Hamming Score ('Accuracy' by Keras):\t%.3f" % (100 * keras_acc.result().numpy()))
    print("Hamming Score (= 1 - Hamming Loss):\t%.3f" %  (100 * (1-metrics.hamming_loss(y_true, y_predicted))))
    print("Exact match ratio (Subset Accuracy):\t%.3f" % (100 * metrics.accuracy_score(y_true, y_predicted)))
    print("F1-Score Micro Averaged:\t\t%.3f" % (100 * metrics.f1_score(y_true, y_predicted, average='micro')))
    print("F1-Score Macro Averaged:\t\t%.3f" % (100 * metrics.f1_score(y_true, y_predicted, average='macro')))
    print("F1-Score Weighted Average:\t\t%.3f" % (100 * metrics.f1_score(y_true, y_predicted, average='weighted')))
    print("Precision Score Micro Averaged:\t\t%.3f" % (100 * metrics.precision_score(y_true, y_predicted, average='micro')))
    print("Recall Score Micro Averaged:\t\t%.3f" % (100 * metrics.recall_score(y_true, y_predicted, average='micro')))

def Sequential_compile_train(X_train, y_train, validation_data=None, epochs=15, verbose=1):
    N_features = X_train.shape[1]
    N_labels = y_train.shape[1]

    model = Sequential()
    model.add(Input(shape=(N_features,)))
    # model.add(Dense(units=5000, activation='relu'))
    model.add(Dense(units=1000, activation='relu'))
    model.add(Dense(units=500, activation='relu'))
    # model.add(Dense(units=500, activation='relu'))
    model.add(Dense(units=100, activation='relu'))
    model.add(Dense(units=N_labels, activation='sigmoid'))

    model.summary()

    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])

    history = model.fit(X_train, y_train, epochs=epochs, batch_size=32, verbose=verbose, 
                        validation_data=validation_data)           

    return model, history

def GridSearchCVMultipleFits(estimator, param_grid, scoring, refit, cv, verbose, data_list):
    clfs = []
    index = 1
    for X_train, y_train in data_list:
        clf = GridSearchCV(estimator=estimator, param_grid=param_grid, 
                   cv=cv, scoring=scoring, refit=refit, verbose=verbose)
        if verbose > 0:
            print("[" + str(index) + ". GridSearchCV]")
        clfs.append(clf)
        clf.fit(X_train, y_train)
        index += 1
    
    mean_test_f1_micro = []
    mean_test_f1_weighted = []
    best_score_ = 0
    for clf in clfs:
        mean_test_f1_micro = mean_test_f1_micro +  list(clf.cv_results_['mean_test_f1_micro'])
        mean_test_f1_weighted = mean_test_f1_weighted +  list(clf.cv_results_['mean_test_f1_weighted'])
        if clf.best_score_ > best_score_:
            best_score_ = clf.best_score_
            best_estimator_ = clf.best_estimator_

    result = {
        'mean_test_f1_micro' : mean_test_f1_micro,
        'mean_test_f1_weighted' : mean_test_f1_weighted,
        'best_score_' : best_score_,
        'best_estimator_' : best_estimator_,
    }

    return result

def plot_training_histroy(history, metric='binary_accuracy', validation=True):
    epochs = history.epoch
    loss = history.history['loss']
    if validation:
        validation_loss = history.history['val_loss']

    score = history.history[metric]
    if validation:
        validation_score = history.history['val_' + metric]

    plt.figure(figsize=(15, 5))
    plt.subplot(1, 2, 1) # row 1, col 2 index 1
    plt.title('Loss')
    plt.xlabel('epochs')
    plt.ylabel('loss')
    plt.plot(epochs, loss, c='red', label='training')
    if validation:
        plt.plot(epochs, validation_loss, c='orange', label='validation')
    plt.legend(loc='best')

    plt.subplot(1, 2, 2) # index 2
    plt.title(metric[0].upper() + metric[1:])
    plt.xlabel('epochs')
    plt.ylabel(metric)
    plt.plot(epochs, score, c='red', label='training')
    if validation:
        plt.plot(epochs, validation_score, c='orange', label='validation')
    plt.legend(loc='best')
    
    plt.show()

